{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# =========================\n",
        "# data_generator.py\n",
        "# ========================="
      ],
      "metadata": {
        "id": "LN0bay-h9wGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkIvpAnZ9oDp"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# data_generator.py  (modified to use streams_1k.json)\n",
        "# =========================\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Generator, Dict, Any\n",
        "\n",
        "\n",
        "class IncidentGenerator:\n",
        "    def __init__(self, data_path: str = \"streams_1k.json\"):\n",
        "        \"\"\"Initialize generator by loading the pre-generated streaming JSON file.\"\"\"\n",
        "        with open(data_path, \"r\") as f:\n",
        "            self.events = json.load(f)\n",
        "        self.index = 0\n",
        "        self.total = len(self.events)\n",
        "\n",
        "    def generate_incident(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return one incident from the loaded JSON, emulating on-the-fly behavior.\"\"\"\n",
        "        if self.index >= self.total:\n",
        "            # restart or stop when reaching the end\n",
        "            self.index = 0\n",
        "        event = self.events[self.index]\n",
        "        self.index += 1\n",
        "\n",
        "        # normalize timestamp if needed\n",
        "        if \"timestamp\" not in event:\n",
        "            event[\"timestamp\"] = datetime.utcnow().isoformat()\n",
        "\n",
        "        return event\n",
        "\n",
        "    def stream(self, n: int = 10, delay: float = 0.5) -> Generator[Dict[str, Any], None, None]:\n",
        "        \"\"\"Yield n incidents sequentially, with optional delay to simulate streaming.\"\"\"\n",
        "        for _ in range(min(n, self.total)):\n",
        "            yield self.generate_incident()\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    gen = IncidentGenerator(\"streams_1k.json\")\n",
        "    for evt in gen.stream(n=5, delay=0.1):\n",
        "        print(evt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =========================\n",
        "# model_example_pa.py\n",
        "# ========================="
      ],
      "metadata": {
        "id": "ABjMtb3I-KNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_example_pa.py\n",
        "\"\"\"\n",
        "Improved streaming training using PassiveAggressiveClassifier, stratified replay,\n",
        "dynamic class weighting, PDE-inspired temporal regularization, and feature interactions.\n",
        "\n",
        "Usage:\n",
        "    python model_example_pa.py\n",
        "\n",
        "Notes:\n",
        "- It will try to import IncidentGenerator from data_generator.py and call `.generate()` or `.stream()`.\n",
        "  If not available, it will fall back to reading streams_1k.json directly.\n",
        "- Adjust constants below (BATCH_SIZE, REPLAY_MEMORY, REPLAY_RATIO) to taste.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, Counter\n",
        "import time\n",
        "import os\n",
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "STREAM_PATH = \"streams_1k.json\"\n",
        "BATCH_SIZE = 100\n",
        "REPLAY_MEMORY = 1000   # larger memory for diversity\n",
        "REPLAY_RATIO = 0.4     # fraction of combined batch drawn from replay\n",
        "WARMUP = 100           # samples to warm up scaler\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# PDE-inspired hyperparam\n",
        "PDE_ALPHA = 0.5        # weight of PDE temporal penalty when adjusting sample weights\n",
        "PDE_SMOOTH = 0.1       # smoothing factor for predicted mean history\n",
        "\n",
        "# --------------------\n",
        "# Helper functions\n",
        "# --------------------\n",
        "def incident_to_features(inc):\n",
        "    \"\"\"Create base + interaction features from an incident dict.\"\"\"\n",
        "    severity = float(inc.get(\"severity\", 1))\n",
        "    cpu = float(inc.get(\"cpu_load\", 0.0))\n",
        "    net = float(inc.get(\"net_bytes\", 0))\n",
        "    # interactions / engineered\n",
        "    interaction1 = cpu * severity\n",
        "    interaction2 = cpu * np.log1p(net)\n",
        "    return np.array([severity, cpu, net, interaction1, interaction2], dtype=float)\n",
        "\n",
        "def incident_to_label(inc):\n",
        "    critical = {\"network_anomaly\", \"privilege_escalation\", \"policy_violation\", \"data_exfil\", \"malware_alert\"}\n",
        "    return 1 if inc.get(\"incident_type\") in critical else 0\n",
        "\n",
        "def stratified_sample_from_replay(replay_buffer, k):\n",
        "    \"\"\"Return X, y arrays sampled stratified by class from replay_buffer (list of (x,y)).\"\"\"\n",
        "    if k <= 0 or len(replay_buffer) == 0:\n",
        "        return np.empty((0,)), np.empty((0,))\n",
        "    by_class = {}\n",
        "    for x, y in replay_buffer:\n",
        "        by_class.setdefault(y, []).append((x, y))\n",
        "    classes = sorted(by_class.keys())\n",
        "    per_class = max(1, k // len(classes))\n",
        "    sampled = []\n",
        "    for cls in classes:\n",
        "        pool = by_class[cls]\n",
        "        if len(pool) <= per_class:\n",
        "            sampled += pool\n",
        "        else:\n",
        "            sampled += random.sample(pool, per_class)\n",
        "    # fill up if short\n",
        "    while len(sampled) < k and len(replay_buffer) > 0:\n",
        "        sampled.append(random.choice(replay_buffer))\n",
        "    Xs = np.array([s[0] for s in sampled])\n",
        "    ys = np.array([s[1] for s in sampled])\n",
        "    return Xs, ys\n",
        "\n",
        "def compute_balance_score(y_true, y_pred):\n",
        "    \"\"\"1 - |Recall_0 - Recall_1|, closer to 1 is more balanced.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    # rows=true classes\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        recall0 = cm[0,0] / cm[0].sum() if cm[0].sum() > 0 else 0.0\n",
        "        recall1 = cm[1,1] / cm[1].sum() if cm[1].sum() > 0 else 0.0\n",
        "    return 1.0 - abs(recall0 - recall1), recall0, recall1\n",
        "\n",
        "# --------------------\n",
        "# Try to import user's generator gracefully\n",
        "# --------------------\n",
        "use_generator = False\n",
        "generator = None\n",
        "try:\n",
        "    from data_generator import IncidentGenerator\n",
        "    try:\n",
        "        # try to instantiate with common constructor signatures\n",
        "        generator = IncidentGenerator(json_path=STREAM_PATH)\n",
        "    except TypeError:\n",
        "        try:\n",
        "            generator = IncidentGenerator(data_path=STREAM_PATH)\n",
        "        except TypeError:\n",
        "            try:\n",
        "                generator = IncidentGenerator()\n",
        "            except Exception:\n",
        "                generator = None\n",
        "    if generator is not None:\n",
        "        use_generator = True\n",
        "except Exception:\n",
        "    use_generator = False\n",
        "    generator = None\n",
        "\n",
        "# Fallback: read JSON file directly\n",
        "if not use_generator:\n",
        "    if not os.path.exists(STREAM_PATH):\n",
        "        raise FileNotFoundError(f\"Could not find generator and {STREAM_PATH} missing.\")\n",
        "    with open(STREAM_PATH, \"r\") as f:\n",
        "        events_all = json.load(f)\n",
        "    def stream_iter():\n",
        "        for e in events_all:\n",
        "            yield e\n",
        "else:\n",
        "    # try several method names\n",
        "    if hasattr(generator, \"generate\"):\n",
        "        def stream_iter():\n",
        "            for e in generator.generate():\n",
        "                yield e\n",
        "    elif hasattr(generator, \"stream\"):\n",
        "        def stream_iter():\n",
        "            for e in generator.stream(n=len(getattr(generator, \"events\", [])), delay=0.0):\n",
        "                yield e\n",
        "    else:\n",
        "        # as fallback, try iterating generator if it's an iterable\n",
        "        try:\n",
        "            def stream_iter():\n",
        "                for e in generator:\n",
        "                    yield e\n",
        "        except Exception:\n",
        "            # final fallback: load file\n",
        "            with open(STREAM_PATH, \"r\") as f:\n",
        "                events_all = json.load(f)\n",
        "            def stream_iter():\n",
        "                for e in events_all:\n",
        "                    yield e\n",
        "\n",
        "# --------------------\n",
        "# Model, scaler, replay buffer\n",
        "# --------------------\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "model = SGDClassifier(loss=\"log_loss\", learning_rate=\"optimal\", random_state=42)\n",
        "\n",
        "# model = PassiveAggressiveClassifier(C=0.01, max_iter=1000, random_state=SEED, tol=1e-3)\n",
        "scaler = StandardScaler()\n",
        "replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "\n",
        "# warmup buffer for scaler\n",
        "warmup = []\n",
        "\n",
        "# bookkeeping for PDE temporal regularization\n",
        "prev_mean_pred = None  # running previous mean prediction\n",
        "smoothed_prev_mean = 0.5  # smoothed history\n",
        "\n",
        "# training loop\n",
        "X_batch_raw, y_batch = [], []\n",
        "batch_count = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for i, inc in enumerate(stream_iter()):\n",
        "    x_raw = incident_to_features(inc)\n",
        "    y = incident_to_label(inc)\n",
        "\n",
        "    X_batch_raw.append(x_raw)\n",
        "    y_batch.append(y)\n",
        "\n",
        "    if len(warmup) < WARMUP:\n",
        "        warmup.append(x_raw)\n",
        "\n",
        "    # when batch ready or end\n",
        "    if (i + 1) % BATCH_SIZE == 0:\n",
        "        batch_count += 1\n",
        "        Xb = np.vstack(X_batch_raw)\n",
        "        yb = np.array(y_batch)\n",
        "\n",
        "        # stratified replay sampling\n",
        "        replay_k = int(REPLAY_RATIO * len(Xb))\n",
        "        X_replay, y_replay = stratified_sample_from_replay(list(replay_buffer), replay_k)\n",
        "\n",
        "        if X_replay.shape[0] > 0:\n",
        "            X_comb = np.vstack([Xb, X_replay])\n",
        "            y_comb = np.concatenate([yb, y_replay])\n",
        "        else:\n",
        "            X_comb = Xb.copy()\n",
        "            y_comb = yb.copy()\n",
        "\n",
        "        # feature scaling: fit scaler on warmup if not fitted\n",
        "        if len(warmup) >= WARMUP and not hasattr(scaler, \"mean_\"):\n",
        "            scaler.fit(np.vstack(warmup))\n",
        "        if hasattr(scaler, \"mean_\"):\n",
        "            X_comb_scaled = scaler.transform(X_comb)\n",
        "            Xb_scaled = scaler.transform(Xb)\n",
        "        else:\n",
        "            X_comb_scaled = X_comb\n",
        "            Xb_scaled = Xb\n",
        "\n",
        "        # dynamic class weights -> sample weights\n",
        "        classes_present = np.unique(y_comb)\n",
        "        if len(classes_present) == 1:\n",
        "            # if missing class, try to augment from replay (or jitter)\n",
        "            missing_class = 0 if classes_present[0] == 1 else 1\n",
        "            # try to get candidates from replay of missing class\n",
        "            candidates = [r for r in list(replay_buffer) if r[1] == missing_class]\n",
        "            if len(candidates) >= 1:\n",
        "                # duplicate some\n",
        "                add_n = min(len(candidates), 5)\n",
        "                addX = np.array([c[0] for c in random.sample(candidates, add_n)])\n",
        "                addy = np.array([c[1] for c in random.sample(candidates, add_n)])\n",
        "                X_comb = np.vstack([X_comb, addX])\n",
        "                y_comb = np.concatenate([y_comb, addy])\n",
        "                # re-scale\n",
        "                if hasattr(scaler, \"mean_\"):\n",
        "                    X_comb_scaled = scaler.transform(X_comb)\n",
        "            else:\n",
        "                # jitter dominant class to synthesize minority examples (last resort)\n",
        "                dom = classes_present[0]\n",
        "                dom_idx = np.where(y_comb == dom)[0]\n",
        "                synth_count = min(5, len(dom_idx))\n",
        "                synth = []\n",
        "                for _ in range(synth_count):\n",
        "                    src = X_comb[ random.choice(dom_idx) ]\n",
        "                    jitter = src + np.random.normal(scale=0.02, size=src.shape)\n",
        "                    synth.append(jitter)\n",
        "                if len(synth) > 0:\n",
        "                    addX = np.vstack(synth)\n",
        "                    addy = np.array([1-dom]*len(synth))\n",
        "                    X_comb = np.vstack([X_comb, addX])\n",
        "                    y_comb = np.concatenate([y_comb, addy])\n",
        "                    if hasattr(scaler, \"mean_\"):\n",
        "                        X_comb_scaled = scaler.transform(X_comb)\n",
        "\n",
        "        # recompute classes present after augmentation\n",
        "        classes_present = np.unique(y_comb)\n",
        "        # compute balanced class weights (per-batch)\n",
        "        try:\n",
        "            cw = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_comb)\n",
        "            weight_dict = {c: w for c, w in zip(classes_present, cw)}\n",
        "            sample_weights = np.array([weight_dict[yy] for yy in y_comb])\n",
        "        except Exception:\n",
        "            # fallback uniform\n",
        "            sample_weights = np.ones(len(y_comb))\n",
        "\n",
        "        # PDE-inspired temporal penalty: compute mean prediction change and adjust sample weights\n",
        "        # get current mean prediction (on combined scaled data)\n",
        "        try:\n",
        "            preds_proba = None\n",
        "            # PassiveAggressive doesn't provide predict_proba; use predict (0/1) mean as proxy\n",
        "            cur_mean_pred = None\n",
        "            if hasattr(model, \"predict\"):\n",
        "                cur_mean_pred = np.mean(model.predict(X_comb_scaled)) if hasattr(model, \"coef_\") else 0.5\n",
        "            else:\n",
        "                cur_mean_pred = 0.5\n",
        "        except Exception:\n",
        "            cur_mean_pred = 0.5\n",
        "\n",
        "        if prev_mean_pred is None:\n",
        "            prev_mean_pred = cur_mean_pred\n",
        "            smoothed_prev_mean = cur_mean_pred\n",
        "\n",
        "        # compute temporal residual\n",
        "        residual = cur_mean_pred - smoothed_prev_mean\n",
        "        # update smoothed_prev_mean with PDE_SMOOTH\n",
        "        smoothed_prev_mean = (1 - PDE_SMOOTH) * smoothed_prev_mean + PDE_SMOOTH * cur_mean_pred\n",
        "\n",
        "        # apply penalty: if residual large, slightly reduce weights of currently dominant class\n",
        "        adjust = np.exp(-PDE_ALPHA * (residual**2))\n",
        "        sample_weights = sample_weights * adjust\n",
        "\n",
        "        # final partial_fit (PassiveAggressive supports partial_fit)\n",
        "        try:\n",
        "            model.partial_fit(X_comb_scaled, y_comb, classes=np.array([0,1]), sample_weight=sample_weights)\n",
        "        except Exception as e:\n",
        "            # in case of any issue, try without sample_weight\n",
        "            model.partial_fit(X_comb_scaled, y_comb, classes=np.array([0,1]))\n",
        "\n",
        "        # update replay buffer with raw (unscaled) Xb and yb\n",
        "        for xr, yr in zip(Xb, yb):\n",
        "            replay_buffer.append((xr, yr))\n",
        "\n",
        "        # evaluation on combined (quick)\n",
        "        y_pred_comb = model.predict(X_comb_scaled)\n",
        "        acc = accuracy_score(y_comb, y_pred_comb)\n",
        "        bal_score, r0, r1 = compute_balance_score(y_comb, y_pred_comb)\n",
        "        rep = classification_report(y_comb, y_pred_comb, digits=3, zero_division=0)\n",
        "        cm = confusion_matrix(y_comb, y_pred_comb, labels=[0,1])\n",
        "\n",
        "        print(f\"\\n--- Batch {batch_count} trained ---\")\n",
        "        print(f\"Batch samples: {len(Xb)} | Combined: {len(X_comb)} | Replay size: {len(replay_buffer)}\")\n",
        "        print(f\"Accuracy (combined): {acc:.3f} | Balance score: {bal_score:.3f} (recalls: {r0:.3f}, {r1:.3f})\")\n",
        "        print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "        print(cm)\n",
        "        print(\"Classification report:\")\n",
        "        print(rep)\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # reset\n",
        "        X_batch_raw, y_batch = [], []\n",
        "\n",
        "        # update prev_mean_pred for next iteration\n",
        "        prev_mean_pred = cur_mean_pred\n",
        "\n",
        "    # optional early stop for debugging\n",
        "    if i > 5000:\n",
        "        break\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training finished in {end_time - start_time:.2f}s\")\n",
        "\n",
        "# Save final model & scaler\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(model, \"model_pa.joblib\")\n",
        "    joblib.dump(scaler, \"scaler_pa.joblib\")\n",
        "    print(\"Saved model_pa.joblib and scaler_pa.joblib\")\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "_FMcNpGt9ttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =========================\n",
        "# evaluation.py\n",
        "# ========================="
      ],
      "metadata": {
        "id": "XnJFOdNv-po-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# evaluation.py\n",
        "# =========================\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class IncidentGenerator:\n",
        "    def __init__(self, json_path=\"streams_1k.json\", noise=False, stream_delay=0.0):\n",
        "        \"\"\"\n",
        "        Load pre-generated streaming incidents.\n",
        "        noise: adds small gaussian noise to numeric features for variability.\n",
        "        stream_delay: optional delay between yielded samples.\n",
        "        \"\"\"\n",
        "        with open(json_path, \"r\") as f:\n",
        "            self.events = json.load(f)\n",
        "\n",
        "        self.noise = noise\n",
        "        self.stream_delay = stream_delay\n",
        "        self.index = 0\n",
        "        self.total = len(self.events)\n",
        "\n",
        "    def generate_incident(self):\n",
        "        \"\"\"Return one incident dict from the dataset.\"\"\"\n",
        "        if self.index >= self.total:\n",
        "            self.index = 0\n",
        "        event = self.events[self.index]\n",
        "        self.index += 1\n",
        "        return event\n",
        "\n",
        "    def stream(self, n=10):\n",
        "        \"\"\"Yield n raw incident dicts sequentially.\"\"\"\n",
        "        for _ in range(min(n, self.total)):\n",
        "            yield self.generate_incident()\n",
        "            if self.stream_delay > 0:\n",
        "                time.sleep(self.stream_delay)\n",
        "\n",
        "    def generate(self):\n",
        "        \"\"\"\n",
        "        Yield (x, y, inc) tuples for model training/evaluation.\n",
        "        x: numpy feature vector\n",
        "        y: label (0=normal, 1=incident)\n",
        "        inc: raw event dict\n",
        "        \"\"\"\n",
        "        for inc in self.events:\n",
        "            # --- feature extraction ---\n",
        "            # اختيار ميزات عددية لتمثيل الحادثة\n",
        "            features = [\n",
        "                inc.get(\"severity\", 0),\n",
        "                inc.get(\"cpu_load\", 0.0),\n",
        "                inc.get(\"net_bytes\", 0),\n",
        "            ]\n",
        "\n",
        "            # إضافة تذبذب بسيط لزيادة تنوع البيانات\n",
        "            if self.noise:\n",
        "                features = [f + random.gauss(0, 0.05) if isinstance(f, (int, float)) else f for f in features]\n",
        "\n",
        "            x = np.array(features, dtype=float)\n",
        "\n",
        "            # --- labeling logic ---\n",
        "            # اعتبر الحالات الخطيرة كـ \"incident\"\n",
        "            y = 1 if inc.get(\"severity\", 1) >= 3 else 0\n",
        "\n",
        "            yield x, y, inc\n",
        "\n",
        "            if self.stream_delay > 0:\n",
        "                time.sleep(self.stream_delay)\n",
        "\n",
        "\n",
        "# --- Quick test ---\n",
        "if __name__ == \"__main__\":\n",
        "    gen = IncidentGenerator(\"streams_1k.json\", noise=True)\n",
        "    for i, (x, y, inc) in enumerate(gen.generate()):\n",
        "        print(f\"{i:03d} | y={y} | x={x} | {inc['incident_type']}\")\n",
        "        if i >= 4:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "f5UdNBDc-8Y8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}