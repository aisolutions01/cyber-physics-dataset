# on_the_fly_dataset/
# ├── README.md
# ├── data_generator.py
# ├── constraints.json
# ├── model_example.py
# ├── evaluation.py
# └── notebooks/
#     └── demo.ipynb

"""
This repo demonstrates On-the-Fly Data Generation for ITSM/SIEM-like use cases.
Inspired by physics-informed PDE analogies, the generator creates synthetic logs
constrained by domain-specific rules. Models are trained and evaluated directly
on streaming synthetic data.
"""

# =========================
# data_generator.py
# =========================
import random
import json
import time
from datetime import datetime

class IncidentGenerator:
    def __init__(self, config_path="constraints.json"):
        with open(config_path, "r") as f:
            self.rules = json.load(f)

    def generate_incident(self):
        incident_type = random.choice(self.rules["incident_types"])
        severity = random.choice(self.rules["severity_levels"])
        timestamp = datetime.utcnow().isoformat()

        if incident_type == "Unauthorized Access":
            severity = "High"

        return {
            "timestamp": timestamp,
            "incident_type": incident_type,
            "severity": severity,
            "source": random.choice(["endpoint-1", "endpoint-2", "server-3"])
        }

    def stream(self, n=10, delay=1.0):
        for _ in range(n):
            yield self.generate_incident()
            time.sleep(delay)

# =========================
# constraints.json (example)
# =========================
# {
#   "incident_types": ["Login Failure", "Unauthorized Access", "Malware Detection", "SLA Violation"],
#   "severity_levels": ["Low", "Medium", "High"]
# }

# =========================
# model_example.py
# =========================
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

from data_generator import IncidentGenerator

def train_model(n_samples=50):
    gen = IncidentGenerator()
    data = [gen.generate_incident() for _ in range(n_samples)]
    df = pd.DataFrame(data)

    df["label"] = df["severity"].apply(lambda x: 1 if x == "High" else 0)

    pipeline = Pipeline([
        ("vec", CountVectorizer()),
        ("clf", LogisticRegression())
    ])

    pipeline.fit(df["incident_type"], df["label"])
    return pipeline, df

if __name__ == "__main__":
    model, dataset = train_model()
    print(dataset.head())
    print("Model trained on-the-fly!")

# =========================
# evaluation.py
# =========================
import pandas as pd
from sklearn.metrics import classification_report
from data_generator import IncidentGenerator

def evaluate_model(model, n_samples=20):
    gen = IncidentGenerator()
    test_data = [gen.generate_incident() for _ in range(n_samples)]
    df = pd.DataFrame(test_data)
    df["label"] = df["severity"].apply(lambda x: 1 if x == "High" else 0)

    preds = model.predict(df["incident_type"])
    print(classification_report(df["label"], preds))

# =========================
# notebooks/demo.ipynb (outline)
# =========================
"""
Notebook demo.ipynb should demonstrate:
1. Importing IncidentGenerator and generating sample incidents.
2. Visualizing a stream of incidents in a table/plot.
3. Training a simple model with model_example.py functions.
4. Evaluating the trained model using evaluation.py.
5. Comparing static dataset vs on-the-fly generated dataset.

Example cells:

from data_generator import IncidentGenerator
from model_example import train_model
from evaluation import evaluate_model

# Generate demo incidents
gen = IncidentGenerator()
for i in range(5):
    print(gen.generate_incident())

# Train model
model, df = train_model(100)
print(df.head())

# Evaluate
from evaluation import evaluate_model
evaluate_model(model, 30)
"""
